 Tumor-cell-detection-and-classification
Visual interpretability of image-based classification models by generative latent space disentanglement applied to in vitro fertilization

<img width="424" alt="{0B5FDB5B-C12D-46AB-97A1-08F86B8E6408}" src="https://github.com/user-attachments/assets/fa08ee3b-f57d-44f2-a78d-7600beee34ef">#
IVF data collection, annotation and ethics  11,211 embryo time-lapse videos were retrospectively collected from IVF cycles conducted at three clinic centers between March 2010 and December 2021. Historical images of blastocyst-stage embryos and metadata were provided by AIVF LTD. All procedures and protocols were approved by an Institutional Review Board for secondary research use (IRB reference number HMO-006-20). This retrospective study using deidentified data followed the guidelines outlined by Declaration of Helsinki for Medical Research involving Human Subjects. This study involves no prospectively collected data. There was no access to patients or requirement for informed consent. Fertilization (time = 0) was determined by the presence of two pronuclei (2PN) 16–18 hours after insemination. All zygotes were placed inside the EmbryoScopeTM time-lapse incubator system (Vitrolife, Denmark), incubated using sequential media protocol until blastocyst-stage, and live imaged with temporal resolution of 15–20 minutes per frame. Each gray-scale image (8 bit) was of size 500×500 pixels, with physical pixel size of 294 × 294 um2. Z-stacks consisting of 7 slices, 15 μm apart, were acquired at each time point, where the middle slice was used for analysis. Analysis was performed for embryos at the blastocyst stage, with typical onset of blastulation occurring ~103 hours post insemination based on manual annotation of blastulation and hatching (end of blastulation). 6–10 frames from embryos at the blastocyst stage were collected with an equal time interval between them. High saturated images and images with a partially visible blastocyst were excluded. Overall, approximately 67,000 images were used to train DISCOVER. Blastocysts were manually annotated by embryologists, just before hatching or before the removal of the embryo from the microscope, according to the Gardner and Schoolcraft (known as “Gardner”) scoring criteria, one of the most common morphologybased blastocyst assessment criteria38. The Gardner criteria is based on three morphology-based quality parameters: (Fig. 1A): Blastocyst expansion status – volume and degree of expansion of the blastocyst cavity (graded 1-6); inner cell mass (ICM) morphology – size and degree of compaction of the mass of cells eventually forming into the fetus (graded A-C); and Trophectoderm (TE) morphology – number and cohesiveness of the single cell layer surround the outer blastocyst eventually forming into the placenta26,94 (graded A-C). Blastocyst expansion status was not annotated in our dataset. High quality blastocysts were defined by corresponding ICM and TE labels of AA, AB, or BA, low quality blastocysts by BB, BC, or CB.  Data preprocessing  The image pixel intensities were normalized to the range [0,1]. To accommodate IVF-CLF training on a single GPU (~30 hours on Nvidia GeForce RTX 3090), the blastocysts images were preprocessed to reduce their size, and following segmentation, non-blastocyst background was masked to reduce irrelevant information. Briefly, the preprocessing steps were (1) semantic segmentation of the blastocyst from the raw image, (2) centering the blastocyst in the image, and (3) resizing the image to a lower resolution. Specifically, we trained a mask-RCNN object detection model95 to detect 200 × 200 pixels bounding boxes around each blastocyst, using 800 raw images with manually annotated blastocysts’ bounding boxes. Hough-transform96 detected the blastocyst circular shape within the mask-RCNN bounding box and was used to mask the non-blastocyst image regions and to center the blastocyst in the image. Next, a U-NET97 was trained to segment the blastocyst using 500 out of the 800 images that were successfully segmented by the Hough transform (based on manual assessment). The U-NET architecture consisted of 4 convolutional blocks for the encoder (down sampling) with 32, 64, 128 and 256 filters and 4 convolutional blocks for the generator (up sampling) with opposite number of filters. Each convolutional block included a 2D convolution layer, batch normalization and “relu” activation. Max pooling was used for the encoder blocks and up sampling convolution was used for the decoder blocks. The U-NET outputs a binary mask. At inference, the Mask-RCNN is first applied to the raw 500 × 500 pixels images to output a bounding box localizing the region of the blastocyst. Next, the U-NET uses the localized region and outputs a binary mask, further localizing the blastocyst region. The Hough transform fits a circular contour to the binary mask. This contour mask is multiplied by the Mask-RCNN output to obtain a blastocyst and masked background image. Using the center 2D coordinate of the circular fit, we can center the blastocyst in the image. Finally, the segmented image is resized to 64 × 64 pixels using nearest neighbors interpolation. The preprocessing pipeline is presented in Supplementary Fig. 1. Images where the blastocyst was not segmented well (partially cut or large background area remained) were excluded based on visual inspection. This approach of applying a segmentation to mask background regions before training a classification CNN was used by others in the domain medical imaging98,99, specifically in the context of IVF100–102. Importantly, masking the background (via segmentation) is not a mandatory pre-possessing step for DISCOVER interpretability as demonstrated for the two natural images datasets.  Classification of high- versus low-quality blastocysts  An ImageNet pretrained VGG-19 network39 was fine-tuned by retraining it to discriminate between high- versus low-quality blastocysts (IVF-CLF classifier, Fig. 1C) using a balanced training dataset of 977 high-quality and 977 low-quality blastocysts. Our test dataset was composed of 108 high-quality and 108 lowquality blastocysts. A negligible fraction of 2% of the blastocysts from the training set had a blastocyst from the same cohort in the test set, thus assumed to have the slightest effect on the classification performance and on the interpretability. The IVF-CLF architecture was composed of the VGG-19 feature extraction part, which includes several blocks in which each has a down sample convolution layer followed by batch normalization, ReLU activation and a final flatting layer. The last fully connected layer of the pretrained VGG-19 layer (which predicts the 1000 classes of ImageNet) was replaced with a fully connected 16 node dense layer and an output node dense layer with a sigmoid activation, which corresponds to a probability of a high quality blastocyst (01). The model was compiled with binary cross entropy loss and Adam optimizer with a learning rate of 0.002. The IVF-CLF network was trained for 100 epochs with a batch size of 32. We performed augmentation by altering brightness, flipping, rotating and by adding Gaussian noise. The performance of the IVF-CLF reached a high classification performance of AUC = 0.93, which is comparable to the performance reported for other recent models and datasets for classification of high versus low quality blastocysts (e.g., AUC = 0.98731, AUC > 0.919, AUC > 0.940).  DIsentangled COunterfactual Visual interpretER (DISCOVER) architecture and optimization  DISCOVER was designed toward generative interpretability by simultaneously optimizing the following properties (Fig. 2A-B): high-quality and realistic reconstruction of the latent space (loss #1), smooth and realistic traversal of the latent space through its reconstructed images  (loss #2), domain-specific classification oriented encoding (loss #3), decorrelated latent space (loss #4), counterfactual disentanglement (loss #5), and a classification-driving subset of latent features that correlated with the classifier that is being interpreted (loss #6). More specifically.  Image reconstruction and latent space traversal (losses #1-2)  High-quality and realistic reconstruction and traversal of the latent space was achieved with an adversarial autoencoder103 (AAE) that was optimized toward a lower dimensional embedded representation of blastocyst images by approximating the high-dimensional data distribution of the input images. This embedding, called latent space, generates a compressed representation that faithfully encodes the input blastocyst. Each blastocyst image is encoded to a point in the latent space that can be decoded to reconstruct an image that appears nearly identical to the original input. The adversarial loss forced the encoded latent representation embedding towards an aggregated posterior distribution similar to a normal distribution in order to achieve a stochastic continuous model to sample from during traversal103. The encoder (Supplementary Table 1) and decoder (Supplementary Table 2) networks backbone were based on residual blocks similar to the ones introduced in Resnet50104. The outputs of the last convolutional down sampling block were flattened to a vector followed by a dense layer of 350 dimensions (determined empirically) that defined the latent representation. The discriminator network was composed of six fully connected dense layers (Supplementary Table 3).
